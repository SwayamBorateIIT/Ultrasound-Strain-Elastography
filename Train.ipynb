{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "\n",
    "class Corr_pyTorch(nn.Module):\n",
    "    '''\n",
    "    my implementation of correlation layer using pytorch\n",
    "    note that the Ispeed is much slower than cuda version\n",
    "    '''\n",
    "\n",
    "    def __init__(self, pad_size=4, kernel_size=1, max_displacement=4, stride1=1, stride2=1):\n",
    "        assert pad_size == max_displacement\n",
    "        assert stride1 == stride2 == 1\n",
    "        super().__init__()\n",
    "        self.pad_size = pad_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride1 = stride1\n",
    "        self.stride2 = stride2\n",
    "        self.max_hdisp = max_displacement\n",
    "        self.padlayer = nn.ConstantPad2d(pad_size, 0)\n",
    "\n",
    "    def forward(self, in1, in2):\n",
    "        bz, cn, hei, wid = in1.shape\n",
    "        f1 = F.unfold(in1, kernel_size=self.kernel_size, padding=self.kernel_size // 2, stride=self.stride1)\n",
    "        f2 = F.unfold(in2, kernel_size=self.kernel_size, padding=self.kernel_size // 2,stride=self.stride2)\n",
    "        searching_kernel_size = f2.shape[1]\n",
    "        f2_ = torch.reshape(f2, (bz, searching_kernel_size, hei, wid))\n",
    "        f2_ = torch.reshape(f2_, (bz * searching_kernel_size, hei, wid)).unsqueeze(1)\n",
    "        f2 = F.unfold(f2_, kernel_size=(hei, wid), padding=self.pad_size, stride=self.stride2)\n",
    "        _, kernel_number, window_number = f2.shape\n",
    "        f2_ = torch.reshape(f2, (bz, searching_kernel_size, kernel_number, window_number))\n",
    "        f2_2 = torch.transpose(f2_, dim0=1, dim1=3).transpose(2, 3)\n",
    "        f1_2 = f1.unsqueeze(1)\n",
    "\n",
    "        res = f2_2 * f1_2\n",
    "        res = torch.mean(res, dim=2)\n",
    "        res = torch.reshape(res, (bz, window_number, hei, wid))\n",
    "        return res\n",
    "\n",
    "class WarpingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WarpingLayer, self).__init__()\n",
    "    def forward(self, x, deformation):\n",
    "        B, C, H, W = x.shape\n",
    "        # mesh grid\n",
    "        xx = torch.arange(0, W).view(1, -1).repeat(H, 1)\n",
    "        yy = torch.arange(0, H).view(-1, 1).repeat(1, W)\n",
    "\n",
    "        xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "        yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "        grid = torch.cat((xx, yy), 1).float().to(device=x.device)\n",
    "        vgrid = grid + deformation  # B,2,H,W\n",
    "\n",
    "        vgrid[:, 0, :, :] = 2.0 * vgrid[:, 0, :, :] / max(W - 1, 1) - 1.0\n",
    "        vgrid[:, 1, :, :] = 2.0 * vgrid[:, 1, :, :] / max(H - 1, 1) - 1.0\n",
    "\n",
    "        vgrid = vgrid.permute(0, 2, 3, 1)  # from B,2,H,W -> B,H,W,2，\n",
    "        x_warp = F.grid_sample(x, vgrid, padding_mode='zeros', align_corners=True)\n",
    "        mask = torch.ones(x.size(), requires_grad=False).to(x.device)\n",
    "        mask = F.grid_sample(mask, vgrid, align_corners=True)\n",
    "        mask = (mask >= 1.0).float()\n",
    "        return x_warp * mask\n",
    "\n",
    "\n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, dilation=1, isReLU=True, if_IN=False, IN_affine=False, if_BN=False):\n",
    "    if isReLU:\n",
    "        if if_IN:\n",
    "\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, dilation=dilation,\n",
    "                          padding=((kernel_size - 1) * dilation) // 2, bias=True),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.InstanceNorm2d(out_planes, affine=IN_affine)\n",
    "\n",
    "            )\n",
    "        elif if_BN:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, dilation=dilation,\n",
    "                          padding=((kernel_size - 1) * dilation) // 2, bias=True),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.BatchNorm2d(out_planes, affine=IN_affine)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, dilation=dilation,\n",
    "                          padding=((kernel_size - 1) * dilation) // 2, bias=True),\n",
    "                nn.LeakyReLU(0.1, inplace=True)\n",
    "            )\n",
    "    else:\n",
    "        if if_IN:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, dilation=dilation,\n",
    "                          padding=((kernel_size - 1) * dilation) // 2, bias=True),\n",
    "                nn.InstanceNorm2d(out_planes, affine=IN_affine)\n",
    "            )\n",
    "        elif if_BN:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, dilation=dilation,\n",
    "                          padding=((kernel_size - 1) * dilation) // 2, bias=True),\n",
    "                nn.BatchNorm2d(out_planes, affine=IN_affine)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, dilation=dilation,\n",
    "                          padding=((kernel_size - 1) * dilation) // 2, bias=True)\n",
    "            )\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, stride=stride)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)\n",
    "        self.relu = nn.LeakyReLU(inplace=False, negative_slope=0.1)\n",
    "\n",
    "        num_groups = planes // 8\n",
    "\n",
    "        if norm_fn == 'group':\n",
    "            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n",
    "            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n",
    "            if not stride == 1:\n",
    "                self.norm3 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n",
    "\n",
    "        elif norm_fn == 'batch':\n",
    "            self.norm1 = nn.BatchNorm2d(planes)\n",
    "            self.norm2 = nn.BatchNorm2d(planes)\n",
    "            if not stride == 1:\n",
    "                self.norm3 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        elif norm_fn == 'instance':\n",
    "            self.norm1 = nn.InstanceNorm2d(planes)\n",
    "            self.norm2 = nn.InstanceNorm2d(planes)\n",
    "            if not stride == 1:\n",
    "                self.norm3 = nn.InstanceNorm2d(planes)\n",
    "\n",
    "        elif norm_fn == 'none':\n",
    "            self.norm1 = nn.Sequential()\n",
    "            self.norm2 = nn.Sequential()\n",
    "            if not stride == 1:\n",
    "                self.norm3 = nn.Sequential()\n",
    "\n",
    "        if stride == 1:\n",
    "            self.downsample = None\n",
    "\n",
    "        else:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride), self.norm3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.relu(self.norm1(self.conv1(y)))\n",
    "        y = self.relu(self.norm2(self.conv2(y)))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        return self.relu(x + y)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=1 ,norm_fn='batch', dropout=0.0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.norm_fn = norm_fn\n",
    "        if self.norm_fn == 'group':\n",
    "            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=16)\n",
    "        elif self.norm_fn == 'batch':\n",
    "            self.norm1 = nn.BatchNorm2d(16)\n",
    "        elif self.norm_fn == 'instance':\n",
    "            self.norm1 = nn.InstanceNorm2d(16)\n",
    "        elif self.norm_fn == 'none':\n",
    "            self.norm1 = nn.Sequential()\n",
    "        self.conv1 = nn.Conv2d(input_dim, 16, kernel_size=7, stride=2, padding=3)\n",
    "        self.relu1 =nn.LeakyReLU(inplace=False, negative_slope=0.1)\n",
    "        self.in_planes =16\n",
    "\n",
    "        self.layer1 = self._make_layer(16, stride=1)\n",
    "        self.layer2 = self._make_layer(32, stride=2)\n",
    "        self.layer3 = self._make_layer(64, stride=2)\n",
    "        self.layer4 = self._make_layer(96, stride=2)\n",
    "        self.layer5 = self._make_layer(128, stride=2)\n",
    "        self.layer6= self._make_layer(196, stride=2)\n",
    "\n",
    "        self.dropout = None\n",
    "        if dropout > 0:\n",
    "            self.dropout = nn.Dropout2d(p=dropout)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.InstanceNorm2d, nn.GroupNorm)):\n",
    "                if m.weight is not None:\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, dim, stride=1):\n",
    "        layer1 = ResidualBlock(self.in_planes, dim, self.norm_fn, stride=stride)\n",
    "        layer2 = ResidualBlock(dim, dim, self.norm_fn, stride=1)\n",
    "        layers = (layer1, layer2)\n",
    "        self.in_planes = dim\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "        disp1 = self.layer1(x)\n",
    "        disp2 = self.layer2(disp1)\n",
    "        disp3 = self.layer3(disp2)\n",
    "        disp4 = self.layer4(disp3)\n",
    "        disp5= self.layer5(disp4)\n",
    "        disp6 = self.layer6(disp5)\n",
    "        return disp6, disp5,disp4, disp3, disp2,\n",
    "\n",
    "\n",
    "class Denseblock(nn.Module):\n",
    "\n",
    "    def __init__(self, ch_in, f_channels=(128, 128, 96, 64, 32), out_channel=2):\n",
    "        super(Denseblock, self).__init__()\n",
    "\n",
    "        N = 0\n",
    "        ind = 0\n",
    "        N += ch_in\n",
    "        self.conv1 = conv(N, f_channels[ind])\n",
    "        N += f_channels[ind]\n",
    "\n",
    "        ind += 1\n",
    "        self.conv2 = conv(N, f_channels[ind])\n",
    "        N += f_channels[ind]\n",
    "\n",
    "        ind += 1\n",
    "        self.conv3 = conv(N, f_channels[ind])\n",
    "        N += f_channels[ind]\n",
    "\n",
    "        ind += 1\n",
    "        self.conv4 = conv(N, f_channels[ind])\n",
    "        N += f_channels[ind]\n",
    "\n",
    "        ind += 1\n",
    "        self.conv5 = conv(N, f_channels[ind])\n",
    "        N += f_channels[ind]\n",
    "        self.n_channels = N\n",
    "        ind += 1\n",
    "        self.conv_last = conv(N, out_channel, isReLU=False)\n",
    "        self.channels = (128, 128, 128, 96, 64, 32, 2)\n",
    "        self.convs = nn.Sequential(\n",
    "            conv(self.n_channels+2, self.channels[0], 3, 1, 1),\n",
    "            conv(self.channels[0], self.channels[1], 3, 1, 2),\n",
    "            conv(self.channels[1], self.channels[2], 3, 1, 4),\n",
    "            conv(self.channels[2], self.channels[3], 3, 1, 8),\n",
    "            conv(self.channels[3], self.channels[4], 3, 1, 16),\n",
    "            conv(self.channels[4], self.channels[5], 3, 1, 1),\n",
    "            conv(self.channels[5], self.channels[6], isReLU=False)\n",
    "        )\n",
    "    def forward(self,disp, x):\n",
    "        x1 = torch.cat([self.conv1(x), x], dim=1)\n",
    "        x2 = torch.cat([self.conv2(x1), x1], dim=1)\n",
    "        x3 = torch.cat([self.conv3(x2), x2], dim=1)\n",
    "        x4 = torch.cat([self.conv4(x3), x3], dim=1)\n",
    "        x5 = torch.cat([self.conv5(x4), x4], dim=1)\n",
    "        x_out = self.conv_last(x5)\n",
    "        disp_ = disp + x_out\n",
    "        out=self.convs(torch.cat([x5, disp_], dim=1))\n",
    "        out_=out + x_out\n",
    "        return disp,out_\n",
    "\n",
    "\n",
    "class DICnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DICnet, self).__init__()\n",
    "        self.num_chs = [1, 16, 32, 64, 96, 128, 196]  # 1/2 1/4 1/8 1/16 1/32 1/64\n",
    "        self.feature_pyramid_extractor = Encoder()\n",
    "        self.conv_1x1 = nn.ModuleList([conv(196, 32, kernel_size=1, stride=1, dilation=1),\n",
    "                                       conv(128, 32, kernel_size=1, stride=1, dilation=1),\n",
    "                                       conv(96, 32, kernel_size=1, stride=1, dilation=1),\n",
    "                                       conv(64, 32, kernel_size=1, stride=1, dilation=1),\n",
    "                                       conv(32, 32, kernel_size=1, stride=1, dilation=1)])\n",
    "\n",
    "        self.leakyRELU = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.warping_layer = WarpingLayer()\n",
    "        self.search_range = 4\n",
    "        self.output_level = 4\n",
    "        self.dim_corr = (self.search_range * 2 + 1) ** 2\n",
    "        self.num_ch_in = self.dim_corr + 32 + 2\n",
    "        self.d_channels = (128, 128, 96, 64, 32)\n",
    "        self.correlation_pytorch = Corr_pyTorch(pad_size=self.search_range, kernel_size=1,max_displacement=self.search_range, stride1=1,stride2=1)  # correlation layer using pytorch\n",
    "        self.denseblock = Denseblock(self.num_ch_in, f_channels=self.d_channels)\n",
    "\n",
    "    def upsample2d_as(self,inputs, target_as, mode=\"bilinear\", if_rate=False):\n",
    "        _, _, h, w = target_as.size()\n",
    "        res = F.interpolate(inputs, [h, w], mode=mode, align_corners=True)\n",
    "        if if_rate:\n",
    "            _, _, h_, w_ = inputs.size()\n",
    "            u_scale = (w / w_)\n",
    "            v_scale = (h / h_)\n",
    "            u, v = res.chunk(2, dim=1)\n",
    "            u = u * u_scale\n",
    "            v = v * v_scale\n",
    "            res = torch.cat([u, v], dim=1)\n",
    "        return res\n",
    "    def normalize_features(self, feature_list, normalize, center, moments_across_channels=False,\n",
    "                           moments_across_images=False):\n",
    "        \"\"\"Normalizes feature tensors (e.g., before computing the cost volume).\n",
    "        Args:\n",
    "          feature_list: list of torch tensors, each with dimensions [b, c, h, w]\n",
    "          normalize: bool flag, divide features by their standard deviation\n",
    "          center: bool flag, subtract feature mean\n",
    "          moments_across_channels: bool flag, compute mean and std across channels, 看到UFlow默认是True\n",
    "          moments_across_images: bool flag, compute mean and std across images, 看到UFlow默认是True\n",
    "\n",
    "        Returns:\n",
    "          list, normalized feature_list\n",
    "        \"\"\"\n",
    "        statistics = collections.defaultdict(list)\n",
    "        axes = [1, 2, 3] if moments_across_channels else [2, 3]  # [b, c, h, w]\n",
    "        for feature_image in feature_list:\n",
    "            mean = torch.mean(feature_image, dim=axes, keepdim=True)  # [b,1,1,1] or [b,c,1,1]\n",
    "            variance = torch.var(feature_image, dim=axes, keepdim=True)  # [b,1,1,1] or [b,c,1,1]\n",
    "            statistics['mean'].append(mean)\n",
    "            statistics['var'].append(variance)\n",
    "        if moments_across_images:\n",
    "            statistics['mean'] = ([torch.mean(torch.stack(statistics['mean'], dim=0), dim=(0,))] * len(feature_list))\n",
    "            statistics['var'] = ([torch.var(torch.stack(statistics['var'], dim=0), dim=(0,))] * len(feature_list))\n",
    "        statistics['std'] = [torch.sqrt(v + 1e-16) for v in statistics['var']]\n",
    "        if center:\n",
    "            feature_list = [\n",
    "                f - mean for f, mean in zip(feature_list, statistics['mean'])\n",
    "            ]\n",
    "        if normalize:\n",
    "            feature_list = [f / std for f, std in zip(feature_list, statistics['std'])]\n",
    "\n",
    "        return feature_list\n",
    "    def Update(self, level, flow_1,  feature_1, feature_1_1x1, feature_2):\n",
    "        up_bilinear = self.upsample2d_as(flow_1, feature_1, mode=\"bilinear\", if_rate=True)\n",
    "        if level == 0:\n",
    "            feature_2_warp = feature_2\n",
    "        else:\n",
    "            feature_2_warp = self.warping_layer(feature_2, up_bilinear)\n",
    "        feature_1, feature_2_warp = self.normalize_features((feature_1, feature_2_warp), normalize=True,center=True)\n",
    "        out_corr_1 = self.correlation_pytorch(feature_1, feature_2_warp)\n",
    "        out_corr_relu_1 = self.leakyRELU(out_corr_1)\n",
    "        up_bilinear, res = self.denseblock(up_bilinear ,torch.cat([out_corr_relu_1, feature_1_1x1, up_bilinear], dim=1))\n",
    "        return  up_bilinear+res\n",
    "\n",
    "    def forward(self,img1,img2):\n",
    "        x1 = self.feature_pyramid_extractor(img1)\n",
    "        x2 = self.feature_pyramid_extractor(img2)\n",
    "        b_size, _, h_x1, w_x1, = x1[0].size()\n",
    "        init_dtype = x1[0].dtype\n",
    "        init_device = x1[0].device\n",
    "        disp_f= torch.zeros(b_size, 2, h_x1, w_x1, dtype=init_dtype, device=init_device).float()\n",
    "        feature_level_ls = []\n",
    "        for l, (x1,x2) in enumerate(zip(x1,x2)):\n",
    "            x1_1by1 = self.conv_1x1[l](x1)\n",
    "            feature_level_ls.append((x1, x1_1by1, x2))  # len = 5\n",
    "        for level, (x1, x1_1by1, x2) in enumerate(feature_level_ls):\n",
    "            disp_f = self.Update(level=level, flow_1=disp_f,feature_1=x1, feature_1_1x1=x1_1by1,feature_2=x2)\n",
    "        disp = self.upsample2d_as(disp_f, img1, mode=\"bilinear\", if_rate=True)\n",
    "        return disp\n",
    "\n",
    "class UnDICnet_d(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(UnDICnet_d, self).__init__()\n",
    "        self.DICnet = DICnet()\n",
    "        self.args = args\n",
    "    def forward(self, img1,img2):\n",
    "\n",
    "        d_f_out = self.DICnet(img1,img2)  # forward estimation\n",
    "        d_b_out = self.DICnet(img2,img1)  # backward estimation\n",
    "        output_dict = {}\n",
    "\n",
    "        output_dict['flow_f_out'] = d_f_out\n",
    "        output_dict['flow_b_out'] = d_b_out\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "class UnDICnet_s(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(UnDICnet_s, self).__init__()\n",
    "        self.DICnet = DICnet()\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        d_f_out = self.DICnet(img1, img2)  # forward estimation\n",
    "        return d_f_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches(x, x_wind=143):\n",
    "    kh, dh = (x_wind*2)+1, 1\n",
    "    patches = x.unfold(2, kh, dh)\n",
    "    patches = torch.squeeze(patches,dim=1).permute(0,1,3,2)\n",
    "    return patches\n",
    "\n",
    "def get_strain(disp, x_wind=143):\n",
    "    d = x_wind*2+1\n",
    "    Uxx_list = []\n",
    "    disp = get_patches(disp,x_wind=x_wind)\n",
    "    depthX = torch.linspace(1,d,d)\n",
    "    depthX = torch.stack([depthX,torch.ones_like(depthX)]).float().permute(1,0).cuda()\n",
    "    depthX = depthX.unsqueeze(0).repeat(disp.shape[1],1,1)\n",
    "    XtX = depthX.permute(0,2,1).bmm(depthX)\n",
    "    for i in range(len(disp)):\n",
    "        # Cholesky decomposition\n",
    "        XtY = depthX.permute(0,2,1).bmm(disp[i,...])\n",
    "        betas_cholesky = torch.linalg.solve(XtX, XtY)\n",
    "        Uxx = torch.squeeze(betas_cholesky[:,0,:])\n",
    "        # pad to original size\n",
    "        Uxx_list += [F.pad(Uxx, (0,0,x_wind, x_wind))]\n",
    "    return torch.stack(Uxx_list).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt  # ✅ FIXED\n",
    "import random\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Warping function: using grid_sample to warp an image given a displacement field.\n",
    "# -----------------------------------------------------------------------------\n",
    "def warp(img, flow):\n",
    "    # img: [B, C, H, W], flow: [B, 2, H, W] (displacement in pixel units)\n",
    "    B, C, H, W = img.shape\n",
    "    grid_y, grid_x = torch.meshgrid(torch.arange(0, H, device=img.device),\n",
    "                                    torch.arange(0, W, device=img.device), indexing='ij')\n",
    "    grid = torch.stack((grid_x, grid_y), dim=2).float()  # [H, W, 2]\n",
    "    grid = grid.unsqueeze(0).repeat(B, 1, 1, 1)  # [B, H, W, 2]\n",
    "    # Convert grid to normalized coordinates in [-1, 1]\n",
    "    grid_norm = torch.zeros_like(grid)\n",
    "    grid_norm[:, :, :, 0] = 2.0 * grid[:, :, :, 0] / (W - 1) - 1.0\n",
    "    grid_norm[:, :, :, 1] = 2.0 * grid[:, :, :, 1] / (H - 1) - 1.0\n",
    "    # Normalize flow to the same scale and add\n",
    "    flow_norm = torch.zeros_like(flow)\n",
    "    flow_norm[:, 0, :, :] = flow[:, 0, :, :] * (2.0 / (W - 1))\n",
    "    flow_norm[:, 1, :, :] = flow[:, 1, :, :] * (2.0 / (H - 1))\n",
    "    warped_grid = grid_norm + flow_norm.permute(0, 2, 3, 1)  # [B, H, W, 2]\n",
    "    warped_img = F.grid_sample(img, warped_grid, align_corners=True, padding_mode='border')\n",
    "    return warped_img\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Loss function implementations\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def patch_znssd_loss(I, I_warp, patch_size=32, stride=16, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Computes the patch-based zero-normalized sum of squared differences (ZNSSD)\n",
    "    between image I and its warped version I_warp.\n",
    "    I and I_warp are assumed to be of shape [B, 1, H, W].\n",
    "    \"\"\"\n",
    "    # Extract patches; output shape: [B, patch_size*patch_size, L]\n",
    "    patches_I = F.unfold(I, kernel_size=patch_size, stride=stride)\n",
    "    patches_I_warp = F.unfold(I_warp, kernel_size=patch_size, stride=stride)\n",
    "    # Compute per-patch mean and standard deviation.\n",
    "    mean_I = patches_I.mean(dim=1, keepdim=True)\n",
    "    std_I = patches_I.std(dim=1, keepdim=True) + epsilon\n",
    "    mean_I_warp = patches_I_warp.mean(dim=1, keepdim=True)\n",
    "    std_I_warp = patches_I_warp.std(dim=1, keepdim=True) + epsilon\n",
    "    # Normalize patches\n",
    "    norm_I = (patches_I - mean_I) / std_I\n",
    "    norm_I_warp = (patches_I_warp - mean_I_warp) / std_I_warp\n",
    "    loss = torch.mean((norm_I - norm_I_warp) ** 2)\n",
    "    return loss\n",
    "\n",
    "def smoothness_loss(flow, img):\n",
    "    \"\"\"\n",
    "    Computes an edge-aware smoothness loss on the flow.\n",
    "    flow: [B, 2, H, W]\n",
    "    img: [B, 1, H, W] used to weight the gradients.\n",
    "    \"\"\"\n",
    "    # Calculate gradients of flow along x and y directions\n",
    "    grad_flow_x = torch.abs(flow[:, :, :, 1:] - flow[:, :, :, :-1])\n",
    "    grad_flow_y = torch.abs(flow[:, :, 1:, :] - flow[:, :, :-1, :])\n",
    "    # Calculate image gradients, averaged over channel\n",
    "    grad_img_x = torch.mean(torch.abs(img[:, :, :, 1:] - img[:, :, :, :-1]), dim=1, keepdim=True)\n",
    "    grad_img_y = torch.mean(torch.abs(img[:, :, 1:, :] - img[:, :, :-1, :]), dim=1, keepdim=True)\n",
    "    loss_x = grad_flow_x * torch.exp(-grad_img_x)\n",
    "    loss_y = grad_flow_y * torch.exp(-grad_img_y)\n",
    "    return torch.mean(loss_x) + torch.mean(loss_y)\n",
    "\n",
    "def census_loss(I, I_warp, kernel_size=7):\n",
    "    \"\"\"\n",
    "    Computes a simplified census loss between I and I_warp.\n",
    "    This loss compares the local structure by forming binary descriptors.\n",
    "    \"\"\"\n",
    "    pad = kernel_size // 2\n",
    "    # Extract local patches\n",
    "    patches_I = F.unfold(I, kernel_size=kernel_size, padding=pad)\n",
    "    patches_I_warp = F.unfold(I_warp, kernel_size=kernel_size, padding=pad)\n",
    "    center_idx = (kernel_size * kernel_size) // 2\n",
    "    # Get the center pixel intensity for each patch\n",
    "    center_I = patches_I[:, center_idx:center_idx+1, :]\n",
    "    center_I_warp = patches_I_warp[:, center_idx:center_idx+1, :]\n",
    "    # Form binary descriptors by comparing each pixel with the center pixel\n",
    "    desc_I = torch.sign(patches_I - center_I)\n",
    "    desc_I_warp = torch.sign(patches_I_warp - center_I_warp)\n",
    "    # Compute the (normalized) Hamming distance as the census loss\n",
    "    diff = torch.abs(desc_I - desc_I_warp) / 2.0\n",
    "    return diff.mean()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Dummy Dataset for illustration (replace with real image pairs)\n",
    "# -----------------------------------------------------------------------------\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class BModeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None, folder_list=None):\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Gather list of subfolders that contain the expected images\n",
    "        self.folder_list = [\n",
    "            folder for folder in os.listdir(root_dir)\n",
    "            if os.path.isdir(os.path.join(root_dir, folder))\n",
    "        ]\n",
    "\n",
    "        # Sort folders (assumes folder names are numeric; adjust key if necessary)\n",
    "        self.folder_list = sorted(self.folder_list, key=lambda x: int(x) if x.isdigit() else x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.folder_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder = self.folder_list[idx]\n",
    "        folder_path = os.path.join(self.root_dir, folder)\n",
    "        pre_path = os.path.join(folder_path, \"pre.png\")\n",
    "        post_path = os.path.join(folder_path, \"post.png\")\n",
    "\n",
    "        # Load as grayscale images\n",
    "        pre_img = Image.open(pre_path).convert(\"L\")\n",
    "        post_img = Image.open(post_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            pre_img = self.transform(pre_img)\n",
    "            post_img = self.transform(post_img)\n",
    "        else:\n",
    "            pre_img = transforms.ToTensor()(pre_img)\n",
    "            post_img = transforms.ToTensor()(post_img)\n",
    "\n",
    "        return {'img1': pre_img, 'img2': post_img}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Data Loader\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Define transformation: resize to 256x256, convert to tensor, then normalize.\n",
    "transform = transforms.Compose([\n",
    "  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "root_dir = \"/teamspace/studios/this_studio/final_faulty_denoised_png\"\n",
    "dataset = BModeDataset(root_dir=root_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the model; here args is set to None for simplicity\n",
    "\n",
    "\n",
    "model = UnDICnet_s(args=None)\n",
    "device = 'cuda' \n",
    "\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=0.0002, weight_decay=0.5e-4)\n",
    "# Loss weighting parameters as selected in the paper\n",
    "omega1 = 5\n",
    "omega2 = 1\n",
    "model.train()\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Training setup\n",
    "num_epochs = 300\n",
    "best_loss = float('inf')\n",
    "checkpoint_dir = \"checkpoints_results_faulty_insampled_2sample_smothness3\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Learning rate scheduler (e.g., StepLR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "\n",
    "    # Create dataset and loader using selected folders\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    loop = tqdm(dataloader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "\n",
    "    for batch in loop:\n",
    "        img1 = batch['img1'].to(device)\n",
    "        img2 = batch['img2'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(img1, img2)\n",
    "        flow_f = outputs\n",
    "        img2_warp = warp(img2, flow_f)\n",
    "\n",
    "        l_sim = patch_znssd_loss(img1, img2_warp)\n",
    "        l_s = smoothness_loss(flow_f, img1)\n",
    "        l_c = census_loss(img1, img2_warp)\n",
    "\n",
    "        loss = l_sim + omega1 * l_s + omega2 * l_c\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if (epoch ) % 5 == 0:\n",
    "        plt.imshow(flow_f[0, 1, :, :].cpu().detach().numpy(), cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.title(f\"Flow Y Component at Epoch {epoch+1}\")\n",
    "        plt.show()\n",
    "\n",
    "        strain_map = get_strain(flow_f[:, 1:2, :, :], 143)  # Use y-displacement\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(strain_map[0, 0].cpu().detach().numpy(), cmap='jet')  # or 'jet' if you prefer\n",
    "        plt.title(\"Strain Map (Uxx)\")\n",
    "        # plt.colorbar(label='Strain')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"✅ Saved best model at epoch {epoch+1} with loss {best_loss:.6f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
